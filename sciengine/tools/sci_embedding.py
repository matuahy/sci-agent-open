# sciengine/sci_embedding.py
"""
RAGçš„åµŒå…¥æ¨¡å—ã€‚
PubMed_url â†’ Pmcid_url â†’ å…¨æ–‡ â†’ åˆ‡å— â†’ å‘é‡åº“
embedding model: bioembedding
vector database: chroma
[å•çº¿ç¨‹ï¼Œå°†å…¨æ–‡é¡ºåºåµŒå…¥å‘é‡æ•°æ®åº“]
"""
import os
import json
from typing import List, Dict, Any
import trafilatura
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from sciengine.tools.pubmed_to_pmc import extract_pmc_link_from_pubmed
from sciengine.model.bioembedding_model import BioBERTEmbeddings
from sciengine.model.llm_models import get_chat_model


class Pubmed_RAG:
    """
    - PubMed â†’ PMC è§£æ
    - ä¸‹è½½å…¨æ–‡
    - åˆ›å»ºå‘é‡åº“
    - å¯¹ outline æ‰§è¡Œ RAG
    - ä½¿ç”¨ LLM ç”Ÿæˆå“åº”
    """

    def __init__(self, embedding_path="/root/autodl-tmp/backend/biobert-embeddings"):
        self.embedding = BioBERTEmbeddings(embedding_path)
        self.llm = get_chat_model()
        self.persist_directory = "./chroma_papers"

    # =====================================================================
    # â‘§ ä» search_results ä¸­æå– PubMed URLï¼ˆä½ éœ€è¦çš„åŠŸèƒ½ï¼‰
    # =====================================================================
    def extract_pubmed_urls_from_tasks(self, search_results: List[Dict[str, Any]]) -> List[str]:
        """
        ä» state['search_results'] ä¸­æå–æ‰€æœ‰ PubMed URLã€‚
        search_results çš„ç»“æ„ä¸€èˆ¬ä¸ºï¼š
            [
                {
                    "result": {
                        "papers": [
                            {"url": "..."},
                            ...
                        ]
                    }
                },
                ...
            ]
        """
        urls = []

        for task in search_results:
            try:
                # å¤šé‡å®‰å…¨æ£€æŸ¥
                if (isinstance(task, dict) and
                        isinstance(task.get("result"), dict) and
                        isinstance(task["result"].get("papers"), list)):

                    for paper in task["result"]["papers"]:
                        if (isinstance(paper, dict) and
                                isinstance(paper.get("url"), str) and
                                "pubmed" in paper["url"].lower()):
                            urls.append(paper["url"])

            except Exception as e:
                print(f"âš ï¸ å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                continue

        # å»é‡
        urls = list(set(urls))

        print(f"âœ… å…±æå–åˆ° {len(urls)} æ¡ PubMed é“¾æ¥")
        return urls

    # =====================================================================
    # â‘  PubMed â†’ PMC
    # =====================================================================
    def batch_get_pmcid(self, urls: List[str]) -> List[Dict[str, Any]]:
        results = []
        for url in urls:
            try:
                res = extract_pmc_link_from_pubmed(url)
                if res:
                    results.append(res)
                else:
                    print(f"[WARN] æ— æ³•è§£æ {url}")
            except Exception as e:
                print(f"[ERROR] {url}: {e}")
        return results

    # =====================================================================
    # â‘¡ ä¸‹è½½ PMC å…¨æ–‡
    # =====================================================================
    def get_paper_content(self, pmcid_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        results = []
        for item in pmcid_items:
            pmc_url = item.get("pmc_url")
            print(f"Downloading: {pmc_url}")

            downloaded = trafilatura.fetch_url(pmc_url)
            text = trafilatura.extract(downloaded) if downloaded else None

            results.append({
                "pubmed_url": item.get("pubmed_url"),
                "pmcid": pmc_url,
                "title": item.get("title"),
                "content": text
            })

        # ä¿å­˜åˆ° jsonï¼ˆå¯é€‰ï¼‰
        with open("paper_content.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        return results

    # =====================================================================
    # â‘¢ åˆ›å»ºå‘é‡æ•°æ®åº“
    # =====================================================================
    def create_VDB_fixed(self, papers):
        """
        ä» paper_content.json ä¸­è¯»å–å†…å®¹ï¼Œ
        æ¯ç¯‡æ–‡ç« å•ç‹¬åˆ‡ç‰‡å¹¶å†™å…¥å‘é‡åº“ï¼ˆåœ¨ for å¾ªç¯å†…éƒ¨æ·»åŠ  Chroma.from_textsï¼‰
        """

        # BERT chunk å»ºè®® <= 300 å­—ç¬¦
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=50
        )

        # åˆ›å»ºå‘é‡åº“ç›®å½•
        os.makedirs(self.persist_directory, exist_ok=True)

        # ============================
        # âœ… éå†æ¯ç¯‡æ–‡ç«  å•ç‹¬å†™å…¥ Chroma
        # ============================
        for idx, paper in enumerate(papers):
            content = paper.get("content")
            title = paper.get("title")

            if not content or not isinstance(content, str):
                print(f"[è·³è¿‡] ç¬¬ {idx + 1} ç¯‡æ–‡ç« ï¼ˆæ—  contentï¼‰")
                continue

            print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {title}")

            # åˆ‡ç‰‡
            chunks = splitter.split_text(content)

            if not chunks:
                print("âš ï¸ åˆ‡ç‰‡ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # metadata å¯¹åº”æ¯ä¸ª chunk
            metas = [
                {
                    "pmcid": paper.get("pmcid"),
                    "title": paper.get("title"),
                    "pubmed_url": paper.get("pubmed_url")
                }
                for _ in chunks
            ]

            # âœ… å°† chunks å†™å…¥ Chromaï¼ˆé€ç¯‡å†™å…¥ï¼‰
            Chroma.from_texts(
                texts=chunks,
                metadatas=metas,
                embedding=self.embedding,
                persist_directory=self.persist_directory
            )

            print(f"âœ… å·²å†™å…¥ {len(chunks)} ä¸ª chunk åˆ°å‘é‡åº“")

        print("ğŸ‰ å‘é‡åº“æ„å»ºå®Œæˆï¼ˆé€ç¯‡å†™å…¥æ¨¡å¼ï¼‰!")

    # =====================================================================
    # â‘¢ åˆ›å»ºå‘é‡æ•°æ®åº“ (å·²ä¿®æ”¹ä¸ºä¼˜å…ˆæŒ‰æ®µè½åˆ‡å—)
    # =====================================================================
    def create_VDB_par(self, papers):
        """
        ä» paper_content.json ä¸­è¯»å–å†…å®¹ï¼Œ
        æ¯ç¯‡æ–‡ç« å•ç‹¬åˆ‡ç‰‡å¹¶å†™å…¥å‘é‡åº“ã€‚

        ä¿®æ”¹ï¼šä½¿ç”¨ RecursiveCharacterTextSplitterï¼Œä¼˜å…ˆæŒ‰æ®µè½åˆ†éš”ç¬¦åˆ‡å—ã€‚
        """

        # BERT chunk å»ºè®® <= 300 å­—ç¬¦ã€‚ä½¿ç”¨é€’å½’åˆ‡å—ï¼Œä¼˜å…ˆæŒ‰æ®µè½åˆ‡åˆ†ã€‚
        # separators é¡ºåºï¼šåŒæ¢è¡Œç¬¦ (æ®µè½)ã€å•æ¢è¡Œç¬¦ã€ç©ºæ ¼ã€å­—ç¬¦
        splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", " ", ""],
            chunk_size=500,  # ä»ç„¶ä¿ç•™ï¼Œç”¨äºå¤„ç†è¶…é•¿æ®µè½
            chunk_overlap=75  # ä»ç„¶ä¿ç•™ï¼Œç”¨äºå¤„ç†è¶…é•¿æ®µè½çš„ overlap
        )

        # åˆ›å»ºå‘é‡åº“ç›®å½•
        os.makedirs(self.persist_directory, exist_ok=True)

        # ============================
        # âœ… éå†æ¯ç¯‡æ–‡ç«  å•ç‹¬å†™å…¥ Chroma
        # ============================
        for idx, paper in enumerate(papers):
            content = paper.get("content")
            title = paper.get("title")

            if not content or not isinstance(content, str):
                print(f"[è·³è¿‡] ç¬¬ {idx + 1} ç¯‡æ–‡ç« ï¼ˆæ—  contentï¼‰")
                continue

            print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {title}")

            # åˆ‡ç‰‡ (ç°åœ¨å®ƒä¼šä¼˜å…ˆæŒ‰æ®µè½åˆ‡ç‰‡)
            chunks = splitter.split_text(content)

            if not chunks:
                print("âš ï¸ åˆ‡ç‰‡ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # metadata å¯¹åº”æ¯ä¸ª chunk
            metas = [
                {
                    "pmcid": paper.get("pmcid"),
                    "title": paper.get("title"),
                    "pubmed_url": paper.get("pubmed_url")
                }
                for _ in chunks
            ]

            # âœ… å°† chunks å†™å…¥ Chromaï¼ˆé€ç¯‡å†™å…¥ï¼‰
            Chroma.from_texts(
                texts=chunks,
                metadatas=metas,
                embedding=self.embedding,
                persist_directory=self.persist_directory
            )

            print(f"âœ… å·²å†™å…¥ {len(chunks)} ä¸ª chunk åˆ°å‘é‡åº“")

        print("ğŸ‰ å‘é‡åº“æ„å»ºå®Œæˆï¼ˆé€ç¯‡å†™å…¥æ¨¡å¼ï¼‰!")


    def run_RAG(self, state):
        pubmed_urls = self.extract_pubmed_urls_from_tasks(state["search_results"])
        print("å·²æå– pubmed é“¾æ¥")

        pmcid_urls = self.batch_get_pmcid(pubmed_urls)
        print("å·²æå– pmcid é“¾æ¥")

        paper_content = self.get_paper_content(pmcid_urls)
        print("å·²è·å– paper content")

        self.create_VDB_par(paper_content)
        print("å·²æ„å»ºå‘é‡æ•°æ®åº“")

        state["paper_content"] = paper_content
        state["chroma_dir"] = self.persist_directory
        print("å·²æ›´æ–°state")

        return {"paper_content": state["paper_content"],
                "chroma_dir": state["chroma_dir"]}





import os
import json
from typing import List, Dict, Any
import trafilatura
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.messages import HumanMessage
from pubmed_to_pmc import extract_pmc_link_from_pubmed
from bioembedding import BioBERTEmbeddings
from llm_models import get_chat_model


class Pubmed_RAG:
    """
    - PubMed â†’ PMC è§£æ
    - ä¸‹è½½å…¨æ–‡
    - åˆ›å»ºå‘é‡åº“
    - å¯¹ outline æ‰§è¡Œ RAG
    - ä½¿ç”¨ LLM ç”Ÿæˆå“åº”
    """

    def __init__(self, embedding_path="/root/autodl-tmp/backend/biobert-embeddings"):
        self.embedding = BioBERTEmbeddings(embedding_path)
        self.llm = get_chat_model()
        self.persist_directory = "./chroma_papers"

        
    # =====================================================================
    # â‘§ ä» search_results ä¸­æå– PubMed URLï¼ˆä½ éœ€è¦çš„åŠŸèƒ½ï¼‰
    # =====================================================================
    def extract_pubmed_urls_from_tasks(self, search_results: List[Dict[str, Any]]) -> List[str]:
        """
        ä» state['search_results'] ä¸­æå–æ‰€æœ‰ PubMed URLã€‚
        search_results çš„ç»“æ„ä¸€èˆ¬ä¸ºï¼š
            [
                {
                    "result": {
                        "papers": [
                            {"url": "..."},
                            ...
                        ]
                    }
                },
                ...
            ]
        """
        urls = []

        for task in search_results:
            result = task.get("result", {})
            papers = result.get("papers", [])

            for paper in papers:
                url = paper.get("url", "")
                if isinstance(url, str) and "pubmed" in url.lower():
                    urls.append(url)

        # å»é‡
        urls = list(set(urls))

        print(f"âœ… å…±æå–åˆ° {len(urls)} æ¡ PubMed é“¾æ¥")
        return urls
        
        
    # =====================================================================
    # â‘  PubMed â†’ PMC
    # =====================================================================
    def batch_get_pmcid(self, urls: List[str]) -> List[Dict[str, Any]]:
        results = []
        for url in urls:
            try:
                res = extract_pmc_link_from_pubmed(url)
                if res:
                    results.append(res)
                else:
                    print(f"[WARN] æ— æ³•è§£æ {url}")
            except Exception as e:
                print(f"[ERROR] {url}: {e}")
        return results

    # =====================================================================
    # â‘¡ ä¸‹è½½ PMC å…¨æ–‡
    # =====================================================================
    def get_paper_content(self, pmcid_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        results = []
        for item in pmcid_items:
            pmc_url = item.get("pmc_url")
            print(f"Downloading: {pmc_url}")

            downloaded = trafilatura.fetch_url(pmc_url)
            text = trafilatura.extract(downloaded) if downloaded else None

            results.append({
                "pubmed_url": item.get("pubmed_url"),
                "pmcid": pmc_url,
                "title": item.get("title"),
                "content": text
            })

        # ä¿å­˜åˆ° jsonï¼ˆå¯é€‰ï¼‰
        with open("../../../../../../../EdgeX/paper_content.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        return results

    # =====================================================================
    # â‘¢ åˆ›å»ºå‘é‡æ•°æ®åº“
    # =====================================================================
    def create_VDB_fixed(self,papers):
        """
        ä» paper_content.json ä¸­è¯»å–å†…å®¹ï¼Œ
        æ¯ç¯‡æ–‡ç« å•ç‹¬åˆ‡ç‰‡å¹¶å†™å…¥å‘é‡åº“ï¼ˆåœ¨ for å¾ªç¯å†…éƒ¨æ·»åŠ  Chroma.from_textsï¼‰
        """

        # BERT chunk å»ºè®® <= 300 å­—ç¬¦
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=50
        )

        # åˆ›å»ºå‘é‡åº“ç›®å½•
        os.makedirs(self.persist_directory, exist_ok=True)

        # ============================
        # âœ… éå†æ¯ç¯‡æ–‡ç«  å•ç‹¬å†™å…¥ Chroma
        # ============================
        for idx, paper in enumerate(papers):
            content = paper.get("content")
            title = paper.get("title")

            if not content or not isinstance(content, str):
                print(f"[è·³è¿‡] ç¬¬ {idx+1} ç¯‡æ–‡ç« ï¼ˆæ—  contentï¼‰")
                continue

            print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {title}")

            # åˆ‡ç‰‡
            chunks = splitter.split_text(content)

            if not chunks:
                print("âš ï¸ åˆ‡ç‰‡ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # metadata å¯¹åº”æ¯ä¸ª chunk
            metas = [
                {
                    "pmcid": paper.get("pmcid"),
                    "title": paper.get("title"),
                    "pubmed_url": paper.get("pubmed_url")
                }
                for _ in chunks
            ]

            # âœ… å°† chunks å†™å…¥ Chromaï¼ˆé€ç¯‡å†™å…¥ï¼‰
            Chroma.from_texts(
                texts=chunks,
                metadatas=metas,
                embedding=self.embedding,
                persist_directory=self.persist_directory
            )

            print(f"âœ… å·²å†™å…¥ {len(chunks)} ä¸ª chunk åˆ°å‘é‡åº“")

        print("ğŸ‰ å‘é‡åº“æ„å»ºå®Œæˆï¼ˆé€ç¯‡å†™å…¥æ¨¡å¼ï¼‰!")


    # =====================================================================
    # â‘¢ åˆ›å»ºå‘é‡æ•°æ®åº“ (å·²ä¿®æ”¹ä¸ºä¼˜å…ˆæŒ‰æ®µè½åˆ‡å—)
    # =====================================================================
    def create_VDB_par(self, papers):
        """
        ä» paper_content.json ä¸­è¯»å–å†…å®¹ï¼Œ
        æ¯ç¯‡æ–‡ç« å•ç‹¬åˆ‡ç‰‡å¹¶å†™å…¥å‘é‡åº“ã€‚
        
        ä¿®æ”¹ï¼šä½¿ç”¨ RecursiveCharacterTextSplitterï¼Œä¼˜å…ˆæŒ‰æ®µè½åˆ†éš”ç¬¦åˆ‡å—ã€‚
        """

        # BERT chunk å»ºè®® <= 300 å­—ç¬¦ã€‚ä½¿ç”¨é€’å½’åˆ‡å—ï¼Œä¼˜å…ˆæŒ‰æ®µè½åˆ‡åˆ†ã€‚
        # separators é¡ºåºï¼šåŒæ¢è¡Œç¬¦ (æ®µè½)ã€å•æ¢è¡Œç¬¦ã€ç©ºæ ¼ã€å­—ç¬¦
        splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", " ", ""], 
            chunk_size=500,  # ä»ç„¶ä¿ç•™ï¼Œç”¨äºå¤„ç†è¶…é•¿æ®µè½
            chunk_overlap=75 # ä»ç„¶ä¿ç•™ï¼Œç”¨äºå¤„ç†è¶…é•¿æ®µè½çš„ overlap
        )

        # åˆ›å»ºå‘é‡åº“ç›®å½•
        os.makedirs(self.persist_directory, exist_ok=True)

        # ============================
        # âœ… éå†æ¯ç¯‡æ–‡ç«  å•ç‹¬å†™å…¥ Chroma
        # ============================
        for idx, paper in enumerate(papers):
            content = paper.get("content")
            title = paper.get("title")

            if not content or not isinstance(content, str):
                print(f"[è·³è¿‡] ç¬¬ {idx+1} ç¯‡æ–‡ç« ï¼ˆæ—  contentï¼‰")
                continue

            print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {title}")

            # åˆ‡ç‰‡ (ç°åœ¨å®ƒä¼šä¼˜å…ˆæŒ‰æ®µè½åˆ‡ç‰‡)
            chunks = splitter.split_text(content)

            if not chunks:
                print("âš ï¸ åˆ‡ç‰‡ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # metadata å¯¹åº”æ¯ä¸ª chunk
            metas = [
                {
                    "pmcid": paper.get("pmcid"),
                    "title": paper.get("title"),
                    "pubmed_url": paper.get("pubmed_url")
                }
                for _ in chunks
            ]

            # âœ… å°† chunks å†™å…¥ Chromaï¼ˆé€ç¯‡å†™å…¥ï¼‰
            Chroma.from_texts(
                texts=chunks,
                metadatas=metas,
                embedding=self.embedding,
                persist_directory=self.persist_directory
            )

            print(f"âœ… å·²å†™å…¥ {len(chunks)} ä¸ª chunk åˆ°å‘é‡åº“")

        print("ğŸ‰ å‘é‡åº“æ„å»ºå®Œæˆï¼ˆé€ç¯‡å†™å…¥æ¨¡å¼ï¼‰!")

        
        
    # =====================================================================
    # â‘£ ä¸€ä¸ªæ–‡æœ¬æ‰§è¡Œ RAG æŸ¥è¯¢
    # =====================================================================
    def rag_query(self, text: str, k=3):
        if not text.strip():
            return []
        db = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embedding
        )
        results = db.similarity_search_with_score(text, k=k)

        formatted = []
        for doc, score in results:
            formatted.append({
                "text": doc.page_content,
                "score": score,
                "metadata": doc.metadata
            })
        return formatted

    # =====================================================================
    # â‘¤ å¯¹ outline æ‰§è¡Œ RAG æŸ¥è¯¢
    # =====================================================================
    def query_outline(self, state: Dict[str, Any]) -> Dict[str, Any]:
        outline = state["planner_output"]["report_outline"]["section"]
        outline_obj = outline.get("report_outline", outline)

        for section in outline_obj.get("sections", []):
            section["paper_response_title"] = self.rag_query(section.get("title", ""))
            section["paper_response_content"] = self.rag_query(section.get("content", ""))

            for sub in section.get("subsections", []):
                sub["paper_response_title"] = self.rag_query(sub.get("title", ""))
                sub["paper_response_content"] = self.rag_query(sub.get("content", ""))

        outline["report_outline"] = outline_obj
        return outline

    # =====================================================================
    # â‘¥ ç”¨ LLM ç”Ÿæˆæ–‡æœ¬
    # =====================================================================
    def llm_generate(self, query, context):
        prompt = (
            "You are a scientific summarizer.\n\n"
            f"Query: {query}\n\n"
            f"Context:\n{context}\n\n"
            "Write a concise scientific explanation."
        )
        response = self.llm.invoke([HumanMessage(prompt)])
        return response.content

    def _compose_context(self, items):
        out = []
        for it in items:
            meta = it.get("metadata", {})
            src = meta.get("title") or meta.get("pmcid") or meta.get("pubmed_url")
            out.append(f"[{src}]\n{it.get('text','')}")
        return "\n---\n".join(out)

    # =====================================================================
    # â‘¦ å¯¹ outline çš„æ¯ä¸ªæ¡ç›®ç”Ÿæˆ LLM å†…å®¹
    # =====================================================================
    def generate_responses(self, outline: Dict[str, Any]) -> Dict[str, Any]:
        outline_obj = outline.get("report_outline", outline)

        for section in outline_obj.get("sections", []):

            # section-title
            ctx = self._compose_context(section.get("paper_response_title", []))
            section["generate_response_title"] = self.llm_generate(section["title"], ctx)

            # section-content
            ctx = self._compose_context(section.get("paper_response_content", []))
            section["generate_response_content"] = self.llm_generate(
                section.get("content", section["title"]), ctx
            )

            for sub in section.get("subsections", []):
                ctx1 = self._compose_context(sub.get("paper_response_title", []))
                ctx2 = self._compose_context(sub.get("paper_response_content", []))

                sub["generate_response_title"] = self.llm_generate(sub["title"], ctx1)
                sub["generate_response_content"] = self.llm_generate(
                    sub.get("content", sub["title"]), ctx2
                )

        outline["report_outline"] = outline_obj
        return outline
    
    def run_RAG(self, state):
        pubmed_urls = self.extract_pubmed_urls_from_tasks(state["search_results"])
        print("å·²æå– pubmed é“¾æ¥")

        pmcid_urls = self.batch_get_pmcid(pubmed_urls)
        print("å·²æå– pmcid é“¾æ¥")

        paper_content = self.get_paper_content(pmcid_urls)
        print("å·²è·å– paper content")

        self.create_VDB_par(paper_content)
        print("å·²æ„å»ºå‘é‡æ•°æ®åº“")

        state["paper_content"] = paper_content
        state["chroma_dir"] = self.persist_directory
        print("å·²æ›´æ–°state")
        
        return {"paper_content": state["paper_content"],
               "chroma_dir":state["chroma_dir"]}

    
    
    
